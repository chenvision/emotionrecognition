## 带注意力机制的BiLSTM

```mermaid
flowchart TD
    A["Input: Token IDs (B, T)"] --> B["Embedding Layer (B, T, E)"]
    B --> C["BiLSTM Layer (B, T, 2H)"]
    C --> D["Self-Attention Weighted Sum (B, 2H)"]
    D --> E["Dropout Layer"]
    E --> F["Fully Connected Layer (B, num class)"]
    F --> G["Output: Logits"]
```

## TextCNN:

```mermaid
flowchart TD
    A["Input: Token IDs (B, T)"] --> B["Embedding Layer (B, T, E)"]
    B --> C["Unsqueeze (B, 1, T, E)"]
    C --> D1["Conv2D Kernel 3 (B, C1, T-2)"]
    C --> D2["Conv2D Kernel 4 (B, C2, T-3)"]
    C --> D3["Conv2D Kernel 5 (B, C3, T-4)"]
    D1 --> E1["ReLU + MaxPool (B, C1)"]
    D2 --> E2["ReLU + MaxPool (B, C2)"]
    D3 --> E3["ReLU + MaxPool (B, C3)"]
    E1 --> F["Concat Features"]
    E2 --> F
    E3 --> F
    F --> G["Dropout Layer"]
    G --> H["Fully Connected Layer (B, num class)"]
    H --> I["Output: Logits"]

```

## 自注意力机制：

```mermaid
flowchart TD
    A["LSTM Outputs (B, T, 2H)"]
    A --> B["Linear Layer (Attention Score)"]
    B --> C["Softmax (Attention Weights)"]
    A --> D["Transpose (B, 2H, T)"]
    D --> E["Batch MatMul Attention (Weighted Sum)"]
    C --> E
    E --> F["Context Vector (B, 2H)"]

    %% 样式设定
    classDef input fill:#E3F2FD,stroke:#2196F3,stroke-width:2px,color:#0D47A1;
    classDef transform fill:#E8F5E9,stroke:#43A047,stroke-width:2px,color:#1B5E20;
    classDef attention fill:#F3E5F5,stroke:#8E24AA,stroke-width:2px,color:#4A148C;
    classDef output fill:#FFF8E1,stroke:#FBC02D,stroke-width:2px,color:#FF6F00;

    class A input;
    class B,C attention;
    class D,E transform;
    class F output;
```

我发现能预测出积极样本的概率非常大，预测出消极的概率非常小，很多消极词都会误预测为积极，使用交叉熵损失函数，

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20250531112616823.png" alt="image-20250531112616823" style="zoom:50%;" />

## 数据集结构

![image-20250531141204162](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20250531141204162.png)

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20250531141328942.png" alt="image-20250531141328942" style="zoom:33%;" />

## 对数据集的预处理

![image-20250602122348796](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20250602122348796.png)

- **`like`** 情感出现次数最多，样本数量最大。
- 其次是 **`disgust`** 和 **`happiness`**。
- **`fear`** 情感的句子最少。
- 情感类别的分布是不均衡的，少数类别样本很少，可能会影响模型训练（**类别不平衡问题**）。

- 后续训练模型，这种分布不均可能导致模型倾向预测样本多的类别（比如总是预测成 `like`），可以考虑在训练时加权损失函数、过采样/欠采样等方法进行平衡处理。

![image-20250602122542190](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20250602122542190.png)



句子长度分布：

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20250602122829438.png" alt="image-20250602122829438" style="zoom:50%;" />

- 大多数句子的长度集中在**10到30个字符**左右。
- 句子越长，出现次数越少，呈现**右偏分布**（long tail）。
- 超过100字符的长句非常少。

**数据清洗参考**：

- 过短的句子（比如 <5字符）可能信息量少，可考虑过滤。
- 极长句子（>128字符）可能超出模型最大长度，需要截断或丢弃。

**模型输入设计参考**：

- 根据这个分布，**最大长度设为64或128**，覆盖绝大多数句子，兼顾效果和效率。

![image-20250602123106944](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20250602123106944.png)

### **正向情感词云图**（Word Cloud of Positive Emotion）

------

### 图的内容解释：

- 这张图展示的是：**在正向情感（positive）句子中，出现频率最高的词**。
- **词频越高，字体越大**，颜色和位置是随机的，增强可视化效果。

------

### 细节解读：

- 词频最高的词包括：
  - `我`、`了`、`有`、`我们`、`他`、`啊`、`是`、`都`、`今天`、`大家` 等等。
- 大量出现的是**代词、助词**（我、他、了、的），属于自然现象。
- 少量出现了**情感词**（比如：`喜欢`、`爱`、`开心`、`感谢`、`祝福`、`希望`）。

### 补充说明：

- 这种情况在**中文情感分析**中很常见，**高频功能词**会占据主导。
- 如果要更有意义的分析：
  - 可以**去掉停用词**（比如我、你、他、了、的）。
  - 只保留**动词、形容词**（真正有情感色彩的词）。

<img src="C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20250602123336158.png" alt="image-20250602123336158" style="zoom:50%;" />

LSTM初始参数训练：

![image-20250602130113896](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20250602130113896.png)

![image-20250602130157799](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20250602130157799.png)

![image-20250602130208153](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20250602130208153.png)
